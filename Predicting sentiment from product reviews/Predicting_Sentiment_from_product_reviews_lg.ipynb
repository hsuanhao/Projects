{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting sentiment from product reviews\n",
    "\n",
    "\n",
    "We will use product review data from Amazon.com to predict whether the sentiments about a product (from its reviews) are positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we import library we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries to use\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "We will use a dataset consisting of baby product reviews on Amazon.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Planetwise Flannel Wipes</td>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0                           Planetwise Flannel Wipes   \n",
       "1                              Planetwise Wipe Pouch   \n",
       "2                Annas Dream Full Quilt with 2 Shams   \n",
       "3  Stop Pacifier Sucking without tears with Thumb...   \n",
       "4  Stop Pacifier Sucking without tears with Thumb...   \n",
       "\n",
       "                                              review  rating  \n",
       "0  These flannel wipes are OK, but in my opinion ...       3  \n",
       "1  it came early and was not disappointed. i love...       5  \n",
       "2  Very soft and comfortable and warmer than it l...       5  \n",
       "3  This is a product well worth the purchase.  I ...       5  \n",
       "4  All of my kids have cried non-stop when I trie...       5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "datapath = os.path.join(\"dataset\", \"\")\n",
    "filename = \"amazon_baby.csv\"\n",
    "products = pd.read_csv(datapath + filename)\n",
    "\n",
    "# Now, let us see a preview of what the dataset looks like.\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 183531 entries, 0 to 183530\n",
      "Data columns (total 3 columns):\n",
      "name      183213 non-null object\n",
      "review    182702 non-null object\n",
      "rating    183531 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "products.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above information, it's obvious that there are some missing values in `review` column. Let's fill NaN values in the `review` column with empty strings since the NaN values indicate empty reviews. \n",
    "\n",
    "Take a look at some cases with NaN values in the `review` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>SoftPlay Twinkle Twinkle Elmo A Bedtime Book</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Our Baby Girl Memory Book</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>Summer Infant, Ultimate Training Pad - Twin Ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>Safety 1st Deluxe 4-in-1 Bath Station</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>Northstate Superyard Playgate Light Gray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   name review  rating\n",
       "38         SoftPlay Twinkle Twinkle Elmo A Bedtime Book    NaN       5\n",
       "58                            Our Baby Girl Memory Book    NaN       5\n",
       "721   Summer Infant, Ultimate Training Pad - Twin Ma...    NaN       5\n",
       "1050              Safety 1st Deluxe 4-in-1 Bath Station    NaN       1\n",
       "1183           Northstate Superyard Playgate Light Gray    NaN       5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products[products[\"review\"].isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = products.fillna({'review':''})  # fill in N/A's in the review column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name      object\n",
       "review    object\n",
       "rating     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data types\n",
    "products.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the word count vector for each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore a specific example of a baby product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name      The First Years Massaging Action Teether\n",
       "review                    A favorite in our house!\n",
       "rating                                           5\n",
       "Name: 269, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.iloc[269]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will perform 2 simple data transformations:\n",
    "\n",
    "1. Remove punctuation using [Python's built-in](https://docs.python.org/2/library/string.html) string functionality.\n",
    "2. Ignore all reviews with rating = 3. And assign reviews with a rating of 4 or higher to be positive reviews (+1), while the ones with rating of 2 or lower are negative (-1).\n",
    "\n",
    "\n",
    "### Remove punctuation in review\n",
    "\n",
    "In this notebook, we remove all punctuations for the sake of simplicity. A smarter approach to punctuations would preserve phrases such as \"I'd\", \"would've\", \"hadn't\" and so forth.\n",
    "\n",
    "**Important**: In original datasets, in `review` there are some missing values. One has to fill missing values by empty string first, and then do following transformation, or those missing values will cause errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A favorite in our house'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Remove punctuation in reviews\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)\n",
    "products['review_clean'].iloc[269]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sentiments\n",
    "\n",
    "We will **ignore** all reviews with *rating = 3*, since they tend to have a neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ignoring all reviews with rating = 3, we have 166752 reviews now.\n"
     ]
    }
   ],
   "source": [
    "products = products[products['rating'] != 3]\n",
    "print(\"After ignoring all reviews with rating = 3, we have {} reviews now.\".format(len(products)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will assign reviews with a rating of 4 or higher to be *positive* reviews, while the ones with rating of 2 or lower are *negative*. For the sentiment column, we use **+1 for the positive class label** and **-1 for the negative class label**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>I had a hard time finding a second year calend...</td>\n",
       "      <td>5</td>\n",
       "      <td>I had a hard time finding a second year calend...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>I only purchased a second-year calendar for my...</td>\n",
       "      <td>2</td>\n",
       "      <td>I only purchased a secondyear calendar for my ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>I LOVE this calendar for recording events of m...</td>\n",
       "      <td>5</td>\n",
       "      <td>I LOVE this calendar for recording events of m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>Wife loves this calender. Comes with a lot of ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Wife loves this calender Comes with a lot of s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name  \\\n",
       "20  Nature's Lullabies Second Year Sticker Calendar   \n",
       "21  Nature's Lullabies Second Year Sticker Calendar   \n",
       "22  Nature's Lullabies Second Year Sticker Calendar   \n",
       "24  Nature's Lullabies Second Year Sticker Calendar   \n",
       "\n",
       "                                               review  rating  \\\n",
       "20  I had a hard time finding a second year calend...       5   \n",
       "21  I only purchased a second-year calendar for my...       2   \n",
       "22  I LOVE this calendar for recording events of m...       5   \n",
       "24  Wife loves this calender. Comes with a lot of ...       5   \n",
       "\n",
       "                                         review_clean  sentiment  \n",
       "20  I had a hard time finding a second year calend...          1  \n",
       "21  I only purchased a secondyear calendar for my ...         -1  \n",
       "22  I LOVE this calendar for recording events of m...          1  \n",
       "24  Wife loves this calender Comes with a lot of s...          1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['sentiment'] = products['rating'].apply(lambda rating : +1 if rating > 3 else -1)\n",
    "products.iloc[18:22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that the dataset contains an extra column called **sentiment** which is either positive (+1) or negative (-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a train/test split with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(products['review_clean'], \n",
    "                                                    products['sentiment'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in training set is 133401 and number of reviews in test set is 33351.\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of reviews in training set is {0} and number of reviews in test set is {1}.\".format(len(X_train),len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at number of reviews in training dataset for each classes to see whether it's a imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    112270\n",
       "-1     21131\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from about result, the ratio between positive class and negative class is 5:1. It may cause classifier always to classify the input as major class, say positive class in the current case. This is something important to keep in mind.\n",
    "\n",
    "Next, we take a look at the test set to see whether the test set has enough points to form a reasonable estimate of generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    27989\n",
       "-1     5362\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the word count vector for each review\n",
    "\n",
    "We will now compute the word count for each word that appears in the reviews. A vector consisting of word counts is often referred to as **bag-of-word** features. Since most words occur in only a few reviews, word count vectors are sparse. For this reason, scikit-learn and many other tools use sparse matrices to store a collection of word count vectors. Refer to appropriate manuals to produce sparse word count vectors. General steps for extracting word count vectors are as follows:\n",
    "\n",
    "- Learn a vocabulary (set of all words) from the training data. Only the words that show up in the training data will be considered for feature extraction.\n",
    "- Compute the occurrences of the words in each review and collect them into a row vector.\n",
    "- Build a sparse matrix where each row is the word count vector for the corresponding review. Call this matrix X_train_vectorized.\n",
    "- Using the same mapping between words and columns, convert the test data into a sparse matrix X_test_vectorized.\n",
    "\n",
    "The following cell uses [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) in scikit-learn. Notice the token_pattern argument in the constructor. Also, `CountVectorizer` does normalization as a default, `lowercase=True`. That is, **the case has been normalized**, or every term is in lowercase. Hence, words like Skype and SKYPE are counted as the same thing. Case variations are very common that case normalization is usually necessary.\n",
    "\n",
    "Note that if we do not provide an a-priori dictionary and we do not use an analyzer that does some kind of feature selection, then the number of features will be equal to the vocabulary size found by analyzing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Use this token pattern to keep single-letter words\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', min_df=5)\n",
    "\n",
    "# First, learn vocabulary from the training data and assign columns to words\n",
    "# Then convert the training data into a sparse matrix\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Second, convert the test data into a sparse matrix, using the same word-column mapping\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the test data must be transformed in the same way as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zipping', 'zippy', 'zips', 'zipties', 'ziptop', 'zipup', 'zoey', 'zoli', 'zombie', 'zone', 'zones', 'zoo', 'zoom', 'zoomed', 'zooming', 'zooms', 'zooper', 'zoos', 'zulily']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at feature names\n",
    "print(vectorizer.get_feature_names()[-20:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20049\n"
     ]
    }
   ],
   "source": [
    "# number of words we found\n",
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a sparse matrix with large size!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a sentiment classifier with logistic regression\n",
    "\n",
    "We will now use **logistic regression** to create a sentiment classifier on the training data. This model will use the sparse word count matrix (`train_matrix`) as an input and the column **sentiment** of `train_data` as the target. \n",
    "\n",
    "**Note:** Since our input is a sparse matrix with very large size, it may take few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sentiment_model = LogisticRegression(max_iter=1000)\n",
    "sentiment_model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a warning to the effect of \"ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\" It means that after 100 iterations, the algorithm is failed to improve in the last iteration of the run. One way to improve it is to increase the maximum iterations. After using `max_iter=1000`, the warning is removed. But the more iterations to get convergence, the more time it takes! \n",
    "\n",
    "The other difficulty arises as the sentiment model puts too much weight on extremely rare words. One way to rectify this is to apply regularization. Here, we use L2 regularization, the default in `LogisticRegression`. Regularization lessens the effect of extremely rare words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the performance on test data\n",
    "\n",
    "We calculate area under curve (AUC) below to check how well the model we trained can generalize to new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9498234199989235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "pred_proba = sentiment_model.predict_proba(X_test_vectorized)[:,1]\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['dissapointed' 'worst' 'disappointing' 'worthless' 'useless' 'unusable'\n",
      " 'intelligent' 'concept' 'tomorrow' 'poorly']\n",
      "\n",
      "Largest Coefs: \n",
      "['outstanding' 'pleasantly' 'minor' 'rich' 'penny' 'excellent' 'relieved'\n",
      " 'saves' 'entertains' 'amazed']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = sentiment_model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# so the list returned is in order of largest to smallest\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see that those words, dissapointed, worst, disappointing, worthless, useless, concept, poorly, unusable, pointless, and tomorrow, give negative reviews. In contrast, the words, outstanding, pleasantly, excellent, minor, penny, skeptical, rich, worry, awesome, and perfect give posistive reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learn another classifier with TFIDF\n",
    "\n",
    "In \"bag of words\" approach, it treats every document as a collection of individual words and every word in a document as a potentially important keyword of the document. **However, a term occurring in every document isn't useful for classification but in bag of words approach, this kind of terms will have more counts.** Hence, we introduce new approach to extract features according to bag of words in the following, and it can give more weights for those important terms such that the common terms among documents become less important.\n",
    "\n",
    "A very popular representation for text is the product of Term Frequency (TF) and Inverse Document Frequency (IDF), commonly referred to as **TFIDF**. The TFIDF value of a term $t$ in a given document $d$ is given by\n",
    "\n",
    "$$ \\text{TFIDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t), \\tag{1}$$\n",
    "\n",
    "where ${\\rm TF}(t,d)$, term counted within the document, is the term frequency (TF) of term $t$ in the document $d$, and ${\\rm IDF}(t)$ is the inverse document frequency (IDF) defined as\n",
    "\n",
    "$$ \\text{IDF}(t) = 1 + \\log \\left( \\frac{\\text{Total number of documents}}{\\text{Number of documents containing } t}\\right) = 1 - \\log \\left( \\frac{\\text{Number of documents containing } t}{\\text{Total number of documents}}\\right)$$.\n",
    "\n",
    "Let's define f as \n",
    "\n",
    "$$ {\\rm f} = \\frac{\\text{Number of documents containing } t}{\\text{Total number of documents}}$$\n",
    "\n",
    "so IDF leads to\n",
    "\n",
    "$$ \\text{IDF}(t) = 1- \\log {\\rm f}(t)$$\n",
    "\n",
    "and note that $ 0 < {\\rm f}(t) \\leq 1 $. Let's draw IDF($t$) versus f($t$) in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def idf(t):\n",
    "    return 1-np.log(t)\n",
    "\n",
    "t = np.arange(0.001, 1.0, 0.01)\n",
    "plt.plot(t, idf(t), 'r--')\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"IDF(t)\")\n",
    "plt.xlabel(\"f(t)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When ${\\rm f}(t) = 1$, it means that the term $t$ occurs in every document so IDF($t$) leads to 1 such that TFIDF in (1) leads to ${\\rm TF}(t,d)$. In contrast, a term $t$ happens rarely. For retrieval, a user may be looking for that exact word so it may be important. As we can see in above plot, when item $t$ is rare among every document, f($t$) is close to 0 such that IDF($t$) becomes very large, and TFIDF becomes very large or ${\\rm TF}(t,d)$ has much larger weight due to importance. That is, in TFIDF approach, each term has different weight according to its importance. For those common terms among documents, they have weight close to 1; for those rare terms among documents, they have weight much larger than 1. By using this approach, most of stopwords and commom words will become less important since TFIDF is lower than other important terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20013"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "# Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5   \n",
    "vect_tfid = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "len(vect_tfid.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9642225134030848\n"
     ]
    }
   ],
   "source": [
    "X_train_vect_tfid = vect_tfid.transform(X_train)\n",
    "\n",
    "model_tfid = LogisticRegression(max_iter=1000)\n",
    "model_tfid.fit(X_train_vect_tfid, y_train)\n",
    "\n",
    "pred_proba_tfid = model_tfid.predict_proba(vect_tfid.transform(X_test))[:,1]\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, pred_proba_tfid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['emotionally' 'well3' 'differentthe' 'seatupdate' 'monthspros'\n",
      " 'paragraph' 'moreupdate' 'suburbs' 'despised' 'use3']\n",
      "\n",
      "Largest tfidf: \n",
      "['beautiful' 'beatiful' 'useful' 'perfect' 'uacutetil' 'excellent'\n",
      " 'product' 'like' 'nice' 'excelente']\n"
     ]
    }
   ],
   "source": [
    "feature_names_tfid = np.array(vect_tfid.get_feature_names())\n",
    "\n",
    "sorted_tfidf_index = X_train_vect_tfid.max(0).toarray()[0].argsort()\n",
    "\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names_tfid[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names_tfid[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['not' 'disappointed' 'useless' 'returned' 'waste' 'return' 'returning'\n",
      " 'poor' 'disappointing' 'worst']\n",
      "\n",
      "Largest Coefs: \n",
      "['love' 'great' 'easy' 'perfect' 'loves' 'best' 'perfectly' 'happy'\n",
      " 'highly' 'glad']\n"
     ]
    }
   ],
   "source": [
    "# Sort the coefficients from the model with tfidf\n",
    "sorted_coef_index_tfidf = model_tfid.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
    "# so the list returned is in order of largest to smallest\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names_tfid[sorted_coef_index_tfidf[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names_tfid[sorted_coef_index_tfidf[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used logistic regression with L2 regularization to predict sentiment from product review by using two different feature vectorizations: **bag of words** and **TFIDF**. The area under the receiver operating characteristic curve (ROC AUC) on test dataset for these two models are summarized below:\n",
    "\n",
    "|  bag of words | TFIDF   |\n",
    "|--------------|----------|\n",
    "| 0.9498       | 0.9642   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
